<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Python应用の简单的爬虫 | Cubernet&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="用Python实现的一个简单爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="Python应用の简单的爬虫">
<meta property="og:url" content="http://cubernet.cn/blog/python-spider/index.html">
<meta property="og:site_name" content="Cubernet's Blog">
<meta property="og:description" content="用Python实现的一个简单爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python应用の简单的爬虫">
<meta name="twitter:description" content="用Python实现的一个简单爬虫">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/blog/css/style.css" type="text/css">
  <link rel="stylesheet" href="/blog/font-awesome/css/font-awesome.min.css" type="text/css">
  


<!-- baidu Analytics -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cbf651a0c9c199e73857441cc83b3884";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- End baidu Analytics -->



</head>

<body style="background:#fff">


  <div id="container">

      

        

<header class="print-invisible post-header-position">
  <div id="header-main" class="header-inner">
    <div class="outer">
      <a href="/blog/" id="post-logo"><i class="logo"></i><span class="site-title">Cubernet&#39;s Blog</span></a>
      <nav id="post-main-nav">
        
          <a class="main-nav-link" href="http://cubernet.cn">Home</a>
        
          <a class="main-nav-link" href="/blog/">Blog</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
          <a class="main-nav-link" href="/blog/about">About</a>
        
      </nav>

    </div>
  </div>
  
</header>



        <div class="outer">

        <section id="post-main"><article id="post-python-spider" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Python应用の简单的爬虫
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/blog/python-spider/">
    <time datetime="2013-12-25T16:39:16.000Z" itemprop="datePublished">2013-12-26</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/blog/categories/技术归档/">技术归档</a>
  </div>

          
  
    <div class="article-category">
  	<i class="fa fa-eye"></i>
  	<span id="busuanzi_value_page_pv"><i class="fa fa-spinner"></i></span>次阅读
  </div>
  

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      
        <div id="toc" class="toc-article">
          <!-- # <strong class="toc-title">contents</strong> -->
          <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#首先，我们要知道如何使用Python获取特定url对应的网页内容。"><span class="toc-number">1.</span> <span class="toc-text">首先，我们要知道如何使用Python获取特定url对应的网页内容。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Http异常处理"><span class="toc-number">2.</span> <span class="toc-text">Http异常处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#知道了如何获取网页内容，接下来需要做的工作就是如何从爬取到的网页代码中找到自己需要的下级url。"><span class="toc-number">3.</span> <span class="toc-text">知道了如何获取网页内容，接下来需要做的工作就是如何从爬取到的网页代码中找到自己需要的下级url。</span></a></li></ol>
        </div>
      
        <p>如果想实现一个简单的爬虫，我想Python一定是最容易上手的。这里就介绍一下如何用Python实现一个简单的爬虫。</p>
<p>爬虫最主要的处理对象就是url，其通常的工作流程就是通过给定的入口url爬取网页内容，然后从得到的网页中寻找下一级的url，接着继续重复以上的工作，直到达到包含所需内容的页面，提取到我们关心的数据。</p>
<blockquote>
<p>网络蜘蛛（Web spider）也叫网络爬虫（Web crawler），指的是“自动化浏览网络”的程序，是网络机器人的一种。这样的电脑程序是为了自动从网络截取特定的数据，或<a id="more"></a>为了组织网络上的数据，所设计的“‘自动浏览网络’的程序”。——维基百科</p>
</blockquote>
<p>所以，我们需要用到哪些知识就很显而易见了。</p>
<hr>
<h2 id="首先，我们要知道如何使用Python获取特定url对应的网页内容。">首先，我们要知道如何使用Python获取特定url对应的网页内容。</h2><p>最简单的，向特定url发送get请求，获取网页内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readsrc</span><span class="params">(src)</span>:</span></span><br><span class="line"><span class="comment">#获取src网址对应的html代码</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        content = urllib2.urlopen(src).read()</span><br><span class="line">        <span class="keyword">return</span> content</span><br><span class="line">    <span class="keyword">except</span> URLError,e:</span><br><span class="line">        <span class="keyword">print</span> e.code</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>有时候服务器会通过http报文的header来判断你是不是一个真正的‘human’，所以需要给你的发送的报文添加一个浏览器标示，来简单的迷惑服务器。同时，很多情况下我们需要通过发送post请求来传递更多的数据，那么可以这么写：</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">user_agent = <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span> </span><br><span class="line">headers = &#123; <span class="string">'User-Agent'</span> : user_agent, <span class="string">'Accept-Language'</span>: <span class="string">':zh-CN,zh;q=0.8,en;q=0.6'</span> &#125; </span><br><span class="line">data = <span class="string">''</span> </span><br><span class="line">	</span><br><span class="line">req = urllib2.Request(geturl2, data, headers)    </span><br><span class="line">response = urllib2.urlopen(req)    </span><br><span class="line">web_page = response.<span class="keyword">read</span>()</span><br></pre></td></tr></table></figure>
<p>如果你的爬虫想爬取的网站需要先登录才可以继续浏览，这个时候你就需要通过处理cookies等字段来维持爬虫保持<code>登录</code>状态。</p>
<figure class="highlight nimrod"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib    </span><br><span class="line"><span class="keyword">import</span> urllib2  </span><br><span class="line"><span class="keyword">import</span> cookielib  </span><br><span class="line">  </span><br><span class="line">cookie = cookielib.<span class="type">CookieJar</span>()    </span><br><span class="line">opener = urllib2.build_opener(urllib2.<span class="type">HTTPCookieProcessor</span>(cookie))  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#需要POST的数据#  </span></span><br><span class="line">postdata=urllib.urlencode(&#123;    </span><br><span class="line">'name':'<span class="type">Cubernet</span>',    </span><br><span class="line">'pwd':'<span class="type">Oppos</span>...'    </span><br><span class="line">&#125;)  </span><br><span class="line">	  </span><br><span class="line"><span class="comment">#自定义一个请求#  </span></span><br><span class="line">req = urllib2.<span class="type">Request</span>(    </span><br><span class="line">url = 'http://cubernet.me/login.php',    </span><br><span class="line">data = postdata  </span><br><span class="line">) </span><br><span class="line">	   </span><br><span class="line"><span class="comment">#访问该链接#  </span></span><br><span class="line"><span class="literal">result</span> = opener.open(req）</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Http异常处理">Http异常处理</h2><p>服务器上每一个HTTP的应答对象response都包含一个数字”状态码”。上面例1中的代码如果出现异常，则会把http异常的状态码答应出来，常见的404等。</p>
<p>HTTP状态码通常分为5种类型，分别以1～5五个数字开头，由3位整数组成：</p>
<pre><code><span class="number">200</span>：请求成功      处理方式：获得响应的内容，进行处理 
<span class="number">201</span>：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到        处理方式：爬虫中不会遇到 
<span class="number">202</span>：请求被接受，但处理尚未完成    处理方式：阻塞等待 
<span class="number">204</span>：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。    处理方式：丢弃
<span class="number">300</span>：该状态码不被HTTP/<span class="number">1.0</span>的应用程序直接使用， 只是作为<span class="number">3</span>XX类型回应的默认解释。存在多个可用的被请求资源。    处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃
<span class="number">301</span>：请求到的资源都会分配一个永久的<span class="built_in">URL</span>，这样就可以在将来通过该<span class="built_in">URL</span>来访问此资源    处理方式：重定向到分配的<span class="built_in">URL</span>
<span class="number">302</span>：请求到的资源在一个不同的<span class="built_in">URL</span>处临时保存     处理方式：重定向到临时的<span class="built_in">URL</span> 
<span class="number">304</span> 请求的资源未更新     处理方式：丢弃 
<span class="number">400</span> 非法请求     处理方式：丢弃 
<span class="number">401</span> 未授权     处理方式：丢弃 
<span class="number">403</span> 禁止     处理方式：丢弃 
<span class="number">404</span> 没有找到     处理方式：丢弃 
<span class="number">5</span>XX 回应代码以“<span class="number">5</span>”开头的状态码表示服务器端发现自己出现错误，不能继续执行请求    处理方式：丢弃
</code></pre><hr>
<h2 id="知道了如何获取网页内容，接下来需要做的工作就是如何从爬取到的网页代码中找到自己需要的下级url。">知道了如何获取网页内容，接下来需要做的工作就是如何从爬取到的网页代码中找到自己需要的下级url。</h2><p>很显然，这里我们需要用到<code>正则表达式</code>这个神奇的工具。</p>
<p>很多时候我们可能觉得正则很鸡肋，学起来知识点比较琐碎，但又没有很明显的用武之地。网上对正则有一个很形象的比喻，“匕首”。所谓匕首，即它不像十八般武器那么炫酷，但是在关键时刻却能起到意想不到的效果。</p>
<p>这里只介绍Python正则表达式中最常用最简单的一种，想更进一步了解的同学可以查阅一下资料，网上有很多总结的不错，我这里就不赘述啦：）</p>
<p><code>re.compile(strPattern[, flag]):</code></p>
<p>这个方法是Pattern类的工厂方法，用于将字符串形式的正则表达式编译为Pattern对象。</p>
<p>第二个参数flag是匹配模式，取值可以使用按位或运算符’|’表示同时生效，比如re.I | re.M。</p>
<p>另外，也可以在regex字符串中指定模式。比如<code>re.compile(&#39;pattern&#39;, re.I | re.M)</code>与<code>re.compile(&#39;(?im)pattern&#39;)</code>是等价的。可选值有：</p>
<pre><code>re.<span class="function"><span class="title">I</span><span class="params">(IGNORECASE)</span></span>        忽略大小写
re.<span class="function"><span class="title">M</span><span class="params">(MULTILINE)</span></span>            多行模式
re.<span class="function"><span class="title">S</span><span class="params">(DOTALL)</span></span>            点任意匹配模式
re.<span class="function"><span class="title">L</span><span class="params">(LOCALE)</span></span>            使预定字符类 \w \W \<span class="tag">b</span> \B \s \S 取决于当前区域设定
re.<span class="function"><span class="title">U</span><span class="params">(UNICODE)</span></span>            使预定字符类 \w \W \<span class="tag">b</span> \B \s \S \d \D 取决于unicode定义的字符属性
re.<span class="function"><span class="title">X</span><span class="params">(VERBOSE)</span></span>            详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。
</code></pre><hr>
<p>最后，想要让你的爬虫足够机智，你必须赋予它灵活的逻辑判断和高效的正则表达式。个人认为赋予爬虫什么样的逻辑才是一个写爬虫真正的精华和难点所在。而这些又是只能依靠个人经验去提升和摸索的。所以还是要多写多练手。</p>
<p>当然，你可以通过增加代理、设置超时、伪装浏览器、反“盗链”等多种手段来提升爬虫的战斗力，但，逻辑依然是它的灵魂所在。</p>

      
    </div>
    

    

    

    <footer class="article-footer">
      <a data-url="http://cubernet.cn/blog/python-spider/" data-id="ciaip9nmg001d1hy9lp6ykp75" class="article-share-link">Share</a>
      
        <a href="http://cubernet.cn/blog/python-spider/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Python/">Python</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav-print">
<div class="article-nav-link-wrap-div-left">
  
      <a href="/blog/Mac/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">Newer</strong>
        <div class="article-nav-title">
          
            Mac相关
          
        </div>
      </a>
  
  </div>
  <div class="article-nav-link-wrap-div-right">
  
    <a href="/blog/Phishing/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Phishing——愿者上钩</div>
    </a>
  
  </div>
</nav>

  
</article>
<div class="print-invisible">
  
<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>

</section>
        
            <aside id="sidebar" class="print-invisible">
            <div id="toTop" class="fa fa-chevron-up"></div>
          </aside>
        

      



    </div>
    <footer id="footer" class="print-invisible">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Cubernet<br>您是本站第<span id="busuanzi_value_site_uv"><i class="fa fa-spinner"></i></span>位访客<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="https://github.com/Cubernet/hexo-theme-EarthShaker/">EarthShaker</a>. 
    </div>
  </div>
</footer>
    

<script>
  var disqus_shortname = 'cubernet';
  var disqus_identifier = '/python-spider/';
  var disqus_title = 'Python应用の简单的爬虫';
  var disqus_url = 'http://cubernet.cn/blog/python-spider/';
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="http://cdn.bootcss.com/jquery/2.1.0/jquery.min.js"></script>
<script src="/blog/js/jquery.githubRepoWidget.min.js"></script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/blog/js/script.js" type="text/javascript"></script>
<script async="true" src="//dn-lbstatics.qbox.me/lbservice/busuanzi/2.0/busuanzi.mini.js"/></script>

  </div>
</body>
</html>